{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#homepage","title":"Homepage","text":""},{"location":"#install-vertex-cli-from-testpypi","title":"Install Vertex-CLI from TestPyPI","text":"<p>Vertex-CLI is a command-line tool that lets you query Large Language Models (LLMs) and debug faster, straight from your terminal.</p> <p>For example, you can run:</p> <pre><code>tex \"tell me about the solar system\"\n</code></pre> <p></p> <p>Replace <code>\"tell me about the solar system\"</code> with any query you like Vertex-CLI will generate a response using your selected LLM model.</p>"},{"location":"#installation","title":"Installation","text":"<p>To install <code>Vertex-CLI</code> from TestPyPI, run:</p> <pre><code>pip install -i https://test.pypi.org/simple/ Vertex-CLI\n</code></pre> <p>Then initialize the CLI with:</p> <pre><code>tex --setup\n</code></pre>"},{"location":"#managing-api-keys-models","title":"Managing API Keys &amp; Models","text":"<ul> <li>Add a model and its API key:</li> </ul> <pre><code>tex config &lt;model-name&gt; &lt;api-key&gt;\n</code></pre> <ul> <li>Remove a model:</li> </ul> <pre><code>tex remove &lt;model-name&gt;\n</code></pre> <ul> <li>List all configured models:</li> </ul> <pre><code>tex list\n</code></pre> <ul> <li>Select a model to use by default:</li> </ul> <pre><code>tex select &lt;model-name&gt;\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Full CLI tool docs: CLI Tool Docs</li> <li>Contributor guide: Contributors Guide</li> <li>API reference: API Reference</li> </ul>"},{"location":"ARCHITECTURE/","title":"Architecture Documentation","text":""},{"location":"ARCHITECTURE/#overview","title":"Overview","text":"<p>Vertex-CLI uses LangChain for unified LLM interactions with a clean, modular architecture following SOLID principles.</p>"},{"location":"ARCHITECTURE/#architecture-layers","title":"Architecture Layers","text":"<pre><code>CLI Layer (prompt.py, llm.py)\n    |\n    v\nService Layer (LLMService, ConfigurationManager)\n    |\n    v\nProvider Layer (ILLMProvider implementations)\n    |\n    v\nLangChain Layer (ChatGoogleGenerativeAI, ChatOpenAI, ChatAnthropic)\n</code></pre>"},{"location":"ARCHITECTURE/#core-components","title":"Core Components","text":""},{"location":"ARCHITECTURE/#1-base-interfaces","title":"1. Base Interfaces","text":"<p>ILLMProvider (<code>cli/models/base.py</code>) - Abstract interface for all LLM providers - Methods: <code>generate()</code>, <code>get_provider_name()</code>, <code>validate_config()</code></p> <p>BaseLLMProvider (<code>cli/models/base_provider.py</code>) - Base implementation with common functionality - Implements DRY principle - Provides: lazy initialization, validation, generation logic</p>"},{"location":"ARCHITECTURE/#2-provider-implementations","title":"2. Provider Implementations","text":"<p>All providers extend <code>BaseLLMProvider</code> and only implement: - <code>_create_llm()</code>: Provider-specific LLM instantiation - <code>get_provider_name()</code>: Return provider name</p> <p>Providers: - <code>GeminiProvider</code>: Google Gemini integration - <code>OpenAIProvider</code>: OpenAI GPT integration - <code>AnthropicProvider</code>: Anthropic Claude integration</p>"},{"location":"ARCHITECTURE/#3-factory-pattern","title":"3. Factory Pattern","text":"<p>LLMProviderFactory (<code>cli/models/factory.py</code>) - Creates provider instances based on configuration - Supports provider registration for extensibility - Auto-detects provider from model name</p>"},{"location":"ARCHITECTURE/#4-service-layer","title":"4. Service Layer","text":"<p>ConfigurationManager (<code>cli/config_manager.py</code>) - Manages model configurations - Storage: <code>~/.config/ai_model_manager/models_config.json</code> - Operations: CRUD for models, selection management</p> <p>LLMService (<code>cli/llm_service.py</code>) - Orchestrates LLM interactions - Provider caching for performance - Dependency injection pattern</p>"},{"location":"ARCHITECTURE/#solid-principles","title":"SOLID Principles","text":""},{"location":"ARCHITECTURE/#single-responsibility","title":"Single Responsibility","text":"<p>Each class has one clear purpose: - <code>ConfigurationManager</code>: Configuration persistence - <code>LLMService</code>: LLM orchestration - Provider classes: Provider-specific logic</p>"},{"location":"ARCHITECTURE/#openclosed","title":"Open/Closed","text":"<ul> <li>Easy to add new providers without modifying existing code</li> <li>Factory pattern enables extension</li> </ul>"},{"location":"ARCHITECTURE/#liskov-substitution","title":"Liskov Substitution","text":"<ul> <li>All providers implement <code>ILLMProvider</code></li> <li>Providers are interchangeable</li> </ul>"},{"location":"ARCHITECTURE/#interface-segregation","title":"Interface Segregation","text":"<ul> <li><code>ILLMProvider</code> defines only essential operations</li> <li>Clean, focused contracts</li> </ul>"},{"location":"ARCHITECTURE/#dependency-inversion","title":"Dependency Inversion","text":"<ul> <li>High-level modules depend on abstractions</li> <li>Dependency injection throughout</li> </ul>"},{"location":"ARCHITECTURE/#configuration-format","title":"Configuration Format","text":"<pre><code>{\n  \"selected_model\": \"gpt-4\",\n  \"models\": {\n    \"gemini-2.5-flash\": {\n      \"provider\": \"google\",\n      \"api_key\": \"your-api-key\",\n      \"temperature\": 0.7,\n      \"max_tokens\": null\n    }\n  }\n}\n</code></pre>"},{"location":"ARCHITECTURE/#adding-new-providers","title":"Adding New Providers","text":"<ol> <li>Create provider class extending <code>BaseLLMProvider</code></li> <li>Implement <code>_create_llm()</code> and <code>get_provider_name()</code></li> <li>Register in <code>LLMProviderFactory</code></li> </ol> <p>Example: <pre><code>from cli.models.base_provider import BaseLLMProvider\n\nclass YourProvider(BaseLLMProvider):\n    def _create_llm(self):\n        return ChatYourProvider(\n            model=self.config.name,\n            api_key=self.config.api_key,\n            temperature=self.config.temperature\n        )\n\n    def get_provider_name(self):\n        return \"Your Provider\"\n</code></pre></p>"},{"location":"ARCHITECTURE/#testing","title":"Testing","text":"<p>Comprehensive test suite: <code>tests/test_vertex.py</code></p> <p>Run tests: <pre><code>python tests/test_vertex.py\n</code></pre></p>"},{"location":"cli_tool_docs/","title":"Vertex CLI","text":"<p>Vertex CLI is a powerful command-line tool that leverages Large Language Models (LLMs) to answer queries and debug faster. With just a few commands, you can set up and start using advanced features like querying LLMs and generating insights.</p> <p>Complete Documentation: Vertex CLI Docs</p>"},{"location":"cli_tool_docs/#installation-and-setup","title":"Installation and Setup","text":"<p>Follow these steps to get started:</p>"},{"location":"cli_tool_docs/#install-vertex-cli-from-testpypi","title":"Install Vertex-CLI from TestPyPI","text":"<p>To install <code>Vertex-CLI</code> from TestPyPI, run:</p> <pre><code>pip install -i https://test.pypi.org/simple/ Vertex-CLI\n</code></pre> <p>After installation, initialize the CLI configuration file:</p> <pre><code>tex --setup\n</code></pre> <p>This will create the <code>models_api.json</code> under <code>~/.config/ai_model_manager/</code> with default entries.</p>"},{"location":"cli_tool_docs/#install-the-editable-version-for-development","title":"Install the Editable Version (For Development)","text":"<p>If you want to modify or contribute to Vertex CLI, install it in editable mode:</p> <ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/Prtm2110/Vertex-CLI\ncd Vertex-CLI\n</code></pre> <ol> <li>Install dependencies and set up the project:</li> </ol> <pre><code>pip install -e .\n</code></pre> <ol> <li>Initialize the CLI configuration:</li> </ol> <pre><code>tex --setup\n</code></pre>"},{"location":"cli_tool_docs/#configuration","title":"Configuration","text":"<p>You can configure the CLI to use a specific LLM model by adding or updating your API key:</p> <pre><code>tex config gemini-1.5-flash YOUR_MODEL_API_KEY\n</code></pre> <p>Replace <code>gemini-1.5-flash</code> with your preferred model name and <code>YOUR_MODEL_API_KEY</code> with your API key.</p> <p>To list all configured models:</p> <pre><code>tex list\n</code></pre> <p>To remove a model:</p> <pre><code>tex remove gemini-1.5-creative\n</code></pre> <p>To select a model as the default:</p> <pre><code>tex select gemini-1.5-flash\n</code></pre>"},{"location":"cli_tool_docs/#usage","title":"Usage","text":"<p>Once installed and configured, you can start chatting or debugging commands:</p>"},{"location":"cli_tool_docs/#quick-command-reference","title":"Quick Command Reference","text":"<ul> <li>Convert an array to a NumPy array</li> </ul> <pre><code>tex \"how to convert an array into a NumPy array\"\n</code></pre> <ul> <li> <p>Manage API keys for models</p> </li> <li> <p>Add or update a model\u2019s API key:</p> <p><pre><code>tex config &lt;model-name&gt; &lt;api-key&gt;\n</code></pre>   * Remove a model:</p> <p><pre><code>tex remove &lt;model-name&gt;\n</code></pre>   * List all saved models and their API keys:</p> <p><pre><code>tex list\n</code></pre>   * Select a model to use:</p> <p><pre><code>tex select &lt;model-name&gt;\n</code></pre>   * Show available commands/help:</p> <pre><code>tex -h\ntex chat -h\n</code></pre> </li> </ul>"},{"location":"cli_tool_docs/#debugging-beta-feature","title":"Debugging (Beta Feature)","text":"<ul> <li>Debug the last 3 commands (default):</li> </ul> <pre><code>tex debug\n</code></pre> <ul> <li>Debug a specific number of recent commands (e.g., last 5):</li> </ul> <pre><code>tex debug -n 5\n</code></pre> <ul> <li>Add a custom message to explain your assumptions or observations:</li> </ul> <pre><code>tex debug -n 5 -p \"I think this issue might be related to environment variables\"\n</code></pre> <p>\ud83d\udd17 Complete CLI Documentation: CLI Commands</p>"},{"location":"contributors_guide/","title":"Installing the Editable Version of Vertex-CLI","text":""},{"location":"contributors_guide/#1-fork-and-clone-the-repository","title":"1. Fork and Clone the Repository","text":"<p>First, fork the repository on GitHub. Then, clone it to your local machine:</p> <pre><code>git clone https://github.com/[YourUserName]/Vertex-CLI\ncd Vertex-CLI\n</code></pre>"},{"location":"contributors_guide/#2-install-in-editable-mode","title":"2. Install in Editable Mode","text":"<p>Run the following command to install the package in editable mode:</p> <pre><code>pip install -e .\n</code></pre> <p>This allows you to make changes to the code and use them immediately without reinstalling.</p>"},{"location":"contributors_guide/#3-initialize-the-cli","title":"3. Initialize the CLI","text":"<p>Run this command to set up the CLI and install necessary dependencies. It will add a free API key for you to get started:</p> <pre><code>tex-init\n</code></pre>"},{"location":"contributors_guide/#4-configure-the-cli-with-your-model-optional","title":"4. Configure the CLI with Your Model (Optional)","text":"<p>If you have your own model API key, you can configure it like this:</p> <pre><code>tex --config gemini-1.5-flash &lt;model-api-key&gt;\n</code></pre> <p>Replace <code>gemini-1.5-flash</code> with your model's name and <code>&lt;model-api-key&gt;</code> with your actual API key.</p>"},{"location":"references/","title":"API Reference","text":""},{"location":"references/#cli.config_manager","title":"<code>config_manager</code>","text":"<p>Configuration manager for AI models.</p>"},{"location":"references/#cli.config_manager.ConfigurationManager","title":"<code>ConfigurationManager</code>","text":"<p>Manages model configurations stored in JSON.</p> Source code in <code>cli/config_manager.py</code> <pre><code>class ConfigurationManager:\n    \"\"\"Manages model configurations stored in JSON.\"\"\"\n\n    def __init__(self, file_path: Optional[str] = None):\n        if file_path is None:\n            config_dir = os.path.join(os.path.expanduser(\"~\"), \".config\", \"ai_model_manager\")\n            os.makedirs(config_dir, exist_ok=True)\n            file_path = os.path.join(config_dir, \"models_config.json\")\n\n        self.file_path = file_path\n\n        if not os.path.exists(self.file_path):\n            self._create_default_config()\n\n    def _create_default_config(self) -&gt; None:\n        \"\"\"Create default configuration file.\"\"\"\n        default_config_path = os.path.join(\n            os.path.dirname(os.path.dirname(__file__)), \"config\", \"default_config.json\"\n        )\n\n        try:\n            with open(default_config_path, \"r\") as f:\n                default_config = json.load(f)\n        except (FileNotFoundError, json.JSONDecodeError):\n            # Fallback if JSON file not found\n            default_config = {\"selected_model\": None, \"models\": {}}\n\n        self._write_config(default_config)\n        print(f\"Configuration file created at: {self.file_path}\")\n\n    def _read_config(self) -&gt; Dict[str, Any]:\n        \"\"\"Read configuration from file.\"\"\"\n        try:\n            with open(self.file_path, \"r\") as f:\n                return json.load(f)\n        except (FileNotFoundError, json.JSONDecodeError):\n            return {\"selected_model\": None, \"models\": {}}\n\n    def _write_config(self, data: Dict[str, Any]) -&gt; None:\n        \"\"\"Write configuration to file.\"\"\"\n        with open(self.file_path, \"w\") as f:\n            json.dump(data, f, indent=4)\n\n    def get_model_config(self, model_name: str) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Get configuration for a specific model.\"\"\"\n        config = self._read_config()\n        return config.get(\"models\", {}).get(model_name)\n\n    def set_model_config(\n        self,\n        model_name: str,\n        provider: str,\n        api_key: str,\n        temperature: float = 0.7,\n        max_tokens: Optional[int] = None,\n    ) -&gt; None:\n        \"\"\"Set or update configuration for a model.\"\"\"\n        config = self._read_config()\n\n        if \"models\" not in config:\n            config[\"models\"] = {}\n\n        config[\"models\"][model_name] = {\n            \"provider\": provider,\n            \"api_key\": api_key,\n            \"temperature\": temperature,\n            \"max_tokens\": max_tokens,\n        }\n\n        self._write_config(config)\n        print(f\"Model '{model_name}' configured successfully.\")\n\n    def remove_model(self, model_name: str) -&gt; None:\n        \"\"\"Remove a model from configuration.\"\"\"\n        config = self._read_config()\n\n        if model_name not in config.get(\"models\", {}):\n            raise ValueError(f\"Model '{model_name}' not found in configuration.\")\n\n        del config[\"models\"][model_name]\n\n        # Clear selected model if it was the removed one\n        if config.get(\"selected_model\") == model_name:\n            config[\"selected_model\"] = None\n\n        self._write_config(config)\n        print(f\"Model '{model_name}' removed successfully.\")\n\n    def get_selected_model(self) -&gt; Optional[str]:\n        \"\"\"Get the currently selected model name.\"\"\"\n        config = self._read_config()\n        return config.get(\"selected_model\")\n\n    def set_selected_model(self, model_name: str) -&gt; None:\n        \"\"\"Set the active model.\"\"\"\n        config = self._read_config()\n        model_config = config.get(\"models\", {}).get(model_name)\n\n        if not model_config:\n            raise ValueError(f\"Model '{model_name}' is not configured.\")\n\n        if not model_config.get(\"api_key\"):\n            raise ValueError(f\"Model '{model_name}' has no API key configured.\")\n\n        config[\"selected_model\"] = model_name\n        self._write_config(config)\n        print(f\"Selected model: {model_name}\")\n\n    def list_models(self) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"Get all configured models.\"\"\"\n        config = self._read_config()\n        return config.get(\"models\", {})\n</code></pre>"},{"location":"references/#cli.config_manager.ConfigurationManager.get_model_config","title":"<code>get_model_config(model_name)</code>","text":"<p>Get configuration for a specific model.</p> Source code in <code>cli/config_manager.py</code> <pre><code>def get_model_config(self, model_name: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Get configuration for a specific model.\"\"\"\n    config = self._read_config()\n    return config.get(\"models\", {}).get(model_name)\n</code></pre>"},{"location":"references/#cli.config_manager.ConfigurationManager.get_selected_model","title":"<code>get_selected_model()</code>","text":"<p>Get the currently selected model name.</p> Source code in <code>cli/config_manager.py</code> <pre><code>def get_selected_model(self) -&gt; Optional[str]:\n    \"\"\"Get the currently selected model name.\"\"\"\n    config = self._read_config()\n    return config.get(\"selected_model\")\n</code></pre>"},{"location":"references/#cli.config_manager.ConfigurationManager.list_models","title":"<code>list_models()</code>","text":"<p>Get all configured models.</p> Source code in <code>cli/config_manager.py</code> <pre><code>def list_models(self) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"Get all configured models.\"\"\"\n    config = self._read_config()\n    return config.get(\"models\", {})\n</code></pre>"},{"location":"references/#cli.config_manager.ConfigurationManager.remove_model","title":"<code>remove_model(model_name)</code>","text":"<p>Remove a model from configuration.</p> Source code in <code>cli/config_manager.py</code> <pre><code>def remove_model(self, model_name: str) -&gt; None:\n    \"\"\"Remove a model from configuration.\"\"\"\n    config = self._read_config()\n\n    if model_name not in config.get(\"models\", {}):\n        raise ValueError(f\"Model '{model_name}' not found in configuration.\")\n\n    del config[\"models\"][model_name]\n\n    # Clear selected model if it was the removed one\n    if config.get(\"selected_model\") == model_name:\n        config[\"selected_model\"] = None\n\n    self._write_config(config)\n    print(f\"Model '{model_name}' removed successfully.\")\n</code></pre>"},{"location":"references/#cli.config_manager.ConfigurationManager.set_model_config","title":"<code>set_model_config(model_name, provider, api_key, temperature=0.7, max_tokens=None)</code>","text":"<p>Set or update configuration for a model.</p> Source code in <code>cli/config_manager.py</code> <pre><code>def set_model_config(\n    self,\n    model_name: str,\n    provider: str,\n    api_key: str,\n    temperature: float = 0.7,\n    max_tokens: Optional[int] = None,\n) -&gt; None:\n    \"\"\"Set or update configuration for a model.\"\"\"\n    config = self._read_config()\n\n    if \"models\" not in config:\n        config[\"models\"] = {}\n\n    config[\"models\"][model_name] = {\n        \"provider\": provider,\n        \"api_key\": api_key,\n        \"temperature\": temperature,\n        \"max_tokens\": max_tokens,\n    }\n\n    self._write_config(config)\n    print(f\"Model '{model_name}' configured successfully.\")\n</code></pre>"},{"location":"references/#cli.config_manager.ConfigurationManager.set_selected_model","title":"<code>set_selected_model(model_name)</code>","text":"<p>Set the active model.</p> Source code in <code>cli/config_manager.py</code> <pre><code>def set_selected_model(self, model_name: str) -&gt; None:\n    \"\"\"Set the active model.\"\"\"\n    config = self._read_config()\n    model_config = config.get(\"models\", {}).get(model_name)\n\n    if not model_config:\n        raise ValueError(f\"Model '{model_name}' is not configured.\")\n\n    if not model_config.get(\"api_key\"):\n        raise ValueError(f\"Model '{model_name}' has no API key configured.\")\n\n    config[\"selected_model\"] = model_name\n    self._write_config(config)\n    print(f\"Selected model: {model_name}\")\n</code></pre>"},{"location":"references/#cli.llm","title":"<code>llm</code>","text":"<p>LLM interaction module.</p>"},{"location":"references/#cli.llm.generate_response","title":"<code>generate_response(prompt, llm_service, history)</code>","text":"<p>Generate and display AI response.</p> Source code in <code>cli/llm.py</code> <pre><code>def generate_response(prompt: str, llm_service: LLMService, history: ChatHistory):\n    \"\"\"Generate and display AI response.\"\"\"\n    system_instruction = (\n        \"System prompt: Give response in short and MD format, \"\n        \"if asked for commands then give commands and don't explain too much\"\n    )\n\n    full_prompt = f\"{prompt}\\n{system_instruction}\"\n    history.append(\"user\", full_prompt)\n\n    # Get conversation history as context\n    flat_prompt = history.get_prompt()\n\n    # Generate response using LLM service\n    response = llm_service.generate(flat_prompt)\n\n    # Save response to history\n    history.append(\"assistant\", response or \"\")\n\n    # Display formatted output\n    prettify_llm_output(response)\n</code></pre>"},{"location":"references/#cli.llm_service","title":"<code>llm_service</code>","text":"<p>Service for LLM generation and orchestration.</p>"},{"location":"references/#cli.llm_service.LLMService","title":"<code>LLMService</code>","text":"<p>Service for generating LLM responses.</p> Source code in <code>cli/llm_service.py</code> <pre><code>class LLMService:\n    \"\"\"Service for generating LLM responses.\"\"\"\n\n    def __init__(self, config_manager: Optional[ConfigurationManager] = None):\n        \"\"\"\n        Initialize LLM service.\n\n        Args:\n            config_manager: Configuration manager instance\n        \"\"\"\n        self.config_manager = config_manager or ConfigurationManager()\n        self._provider_cache: dict[str, ILLMProvider] = {}\n\n    def _get_provider(self, model_name: str) -&gt; ILLMProvider:\n        \"\"\"Get or create a provider instance for the model.\"\"\"\n        if model_name in self._provider_cache:\n            return self._provider_cache[model_name]\n\n        # Get model configuration\n        model_config_dict = self.config_manager.get_model_config(model_name)\n        if not model_config_dict:\n            raise ValueError(f\"Model '{model_name}' is not configured.\")\n\n        if not model_config_dict.get(\"api_key\"):\n            raise ValueError(f\"Model '{model_name}' has no API key configured.\")\n\n        # Create ModelConfig object\n        model_config = ModelConfig(\n            name=model_name,\n            provider=model_config_dict[\"provider\"],\n            api_key=model_config_dict[\"api_key\"],\n            temperature=model_config_dict.get(\"temperature\", 0.7),\n            max_tokens=model_config_dict.get(\"max_tokens\"),\n        )\n\n        # Create provider using factory\n        provider = LLMProviderFactory.create(model_config)\n\n        # Cache the provider\n        self._provider_cache[model_name] = provider\n\n        return provider\n\n    def generate(\n        self,\n        prompt: str,\n        model_name: Optional[str] = None,\n        show_spinner: bool = True,\n        **kwargs,\n    ) -&gt; str:\n        \"\"\"Generate response from LLM.\"\"\"\n        if model_name is None:\n            model_name = self.config_manager.get_selected_model()\n            if not model_name:\n                # Fall back to default\n                model_name = \"gemini-2.5-flash\"\n\n        # Get provider\n        provider = self._get_provider(model_name)\n\n        # Generate with optional spinner\n        if show_spinner:\n            stop_spinner = threading.Event()\n            spinner_thread = threading.Thread(target=spin_loader, args=(stop_spinner,))\n            spinner_thread.start()\n\n            try:\n                response = provider.generate(prompt, **kwargs)\n            finally:\n                stop_spinner.set()\n                spinner_thread.join()\n        else:\n            response = provider.generate(prompt, **kwargs)\n\n        return response\n</code></pre>"},{"location":"references/#cli.llm_service.LLMService.__init__","title":"<code>__init__(config_manager=None)</code>","text":"<p>Initialize LLM service.</p> <p>Parameters:</p> Name Type Description Default <code>config_manager</code> <code>Optional[ConfigurationManager]</code> <p>Configuration manager instance</p> <code>None</code> Source code in <code>cli/llm_service.py</code> <pre><code>def __init__(self, config_manager: Optional[ConfigurationManager] = None):\n    \"\"\"\n    Initialize LLM service.\n\n    Args:\n        config_manager: Configuration manager instance\n    \"\"\"\n    self.config_manager = config_manager or ConfigurationManager()\n    self._provider_cache: dict[str, ILLMProvider] = {}\n</code></pre>"},{"location":"references/#cli.llm_service.LLMService.generate","title":"<code>generate(prompt, model_name=None, show_spinner=True, **kwargs)</code>","text":"<p>Generate response from LLM.</p> Source code in <code>cli/llm_service.py</code> <pre><code>def generate(\n    self,\n    prompt: str,\n    model_name: Optional[str] = None,\n    show_spinner: bool = True,\n    **kwargs,\n) -&gt; str:\n    \"\"\"Generate response from LLM.\"\"\"\n    if model_name is None:\n        model_name = self.config_manager.get_selected_model()\n        if not model_name:\n            # Fall back to default\n            model_name = \"gemini-2.5-flash\"\n\n    # Get provider\n    provider = self._get_provider(model_name)\n\n    # Generate with optional spinner\n    if show_spinner:\n        stop_spinner = threading.Event()\n        spinner_thread = threading.Thread(target=spin_loader, args=(stop_spinner,))\n        spinner_thread.start()\n\n        try:\n            response = provider.generate(prompt, **kwargs)\n        finally:\n            stop_spinner.set()\n            spinner_thread.join()\n    else:\n        response = provider.generate(prompt, **kwargs)\n\n    return response\n</code></pre>"},{"location":"references/#cli.models","title":"<code>models</code>","text":"<p>Models package for LLM abstraction layer. Provides interfaces and implementations for different LLM providers.</p>"},{"location":"references/#cli.models.ILLMProvider","title":"<code>ILLMProvider</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Interface for LLM providers.</p> Source code in <code>cli/models/base.py</code> <pre><code>class ILLMProvider(ABC):\n    \"\"\"Interface for LLM providers.\"\"\"\n\n    @abstractmethod\n    def generate(self, prompt: str, **kwargs) -&gt; str:\n        \"\"\"Generate a response from the LLM.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_provider_name(self) -&gt; str:\n        \"\"\"Get the name of the provider.\"\"\"\n        pass\n\n    @abstractmethod\n    def validate_config(self) -&gt; bool:\n        \"\"\"Validate the model configuration.\"\"\"\n        pass\n</code></pre>"},{"location":"references/#cli.models.ILLMProvider.generate","title":"<code>generate(prompt, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Generate a response from the LLM.</p> Source code in <code>cli/models/base.py</code> <pre><code>@abstractmethod\ndef generate(self, prompt: str, **kwargs) -&gt; str:\n    \"\"\"Generate a response from the LLM.\"\"\"\n    pass\n</code></pre>"},{"location":"references/#cli.models.ILLMProvider.get_provider_name","title":"<code>get_provider_name()</code>  <code>abstractmethod</code>","text":"<p>Get the name of the provider.</p> Source code in <code>cli/models/base.py</code> <pre><code>@abstractmethod\ndef get_provider_name(self) -&gt; str:\n    \"\"\"Get the name of the provider.\"\"\"\n    pass\n</code></pre>"},{"location":"references/#cli.models.ILLMProvider.validate_config","title":"<code>validate_config()</code>  <code>abstractmethod</code>","text":"<p>Validate the model configuration.</p> Source code in <code>cli/models/base.py</code> <pre><code>@abstractmethod\ndef validate_config(self) -&gt; bool:\n    \"\"\"Validate the model configuration.\"\"\"\n    pass\n</code></pre>"},{"location":"references/#cli.models.LLMProviderFactory","title":"<code>LLMProviderFactory</code>","text":"<p>Factory for creating LLM provider instances.</p> Source code in <code>cli/models/factory.py</code> <pre><code>class LLMProviderFactory:\n    \"\"\"Factory for creating LLM provider instances.\"\"\"\n\n    _providers: Dict[str, Type[ILLMProvider]] = {\n        \"google\": GeminiProvider,\n        \"gemini\": GeminiProvider,\n        \"openai\": OpenAIProvider,\n        \"anthropic\": AnthropicProvider,\n        \"claude\": AnthropicProvider,\n    }\n\n    _model_patterns = {\n        \"gemini\": \"google\",\n        \"gpt\": \"openai\",\n        \"claude\": \"anthropic\",\n    }\n\n    @classmethod\n    def create(cls, config: ModelConfig) -&gt; ILLMProvider:\n        \"\"\"Create an LLM provider instance based on configuration.\"\"\"\n        provider_key = config.provider.lower()\n\n        if provider_key in cls._providers:\n            return cls._providers[provider_key](config)\n\n        inferred_provider = cls._infer_provider_from_model(config.name)\n        if inferred_provider:\n            return cls._providers[inferred_provider](config)\n\n        raise ValueError(\n            f\"Unsupported provider: {config.provider}. \"\n            f\"Available: {', '.join(cls.get_supported_providers())}\"\n        )\n\n    @classmethod\n    def _infer_provider_from_model(cls, model_name: str) -&gt; str | None:\n        \"\"\"Infer provider from model name.\"\"\"\n        model_lower = model_name.lower()\n        for pattern, provider in cls._model_patterns.items():\n            if pattern in model_lower:\n                return provider\n        return None\n\n    @classmethod\n    def get_supported_providers(cls) -&gt; list[str]:\n        \"\"\"Get list of supported provider names.\"\"\"\n        return list(set(cls._providers.keys()))\n</code></pre>"},{"location":"references/#cli.models.LLMProviderFactory.create","title":"<code>create(config)</code>  <code>classmethod</code>","text":"<p>Create an LLM provider instance based on configuration.</p> Source code in <code>cli/models/factory.py</code> <pre><code>@classmethod\ndef create(cls, config: ModelConfig) -&gt; ILLMProvider:\n    \"\"\"Create an LLM provider instance based on configuration.\"\"\"\n    provider_key = config.provider.lower()\n\n    if provider_key in cls._providers:\n        return cls._providers[provider_key](config)\n\n    inferred_provider = cls._infer_provider_from_model(config.name)\n    if inferred_provider:\n        return cls._providers[inferred_provider](config)\n\n    raise ValueError(\n        f\"Unsupported provider: {config.provider}. \"\n        f\"Available: {', '.join(cls.get_supported_providers())}\"\n    )\n</code></pre>"},{"location":"references/#cli.models.LLMProviderFactory.get_supported_providers","title":"<code>get_supported_providers()</code>  <code>classmethod</code>","text":"<p>Get list of supported provider names.</p> Source code in <code>cli/models/factory.py</code> <pre><code>@classmethod\ndef get_supported_providers(cls) -&gt; list[str]:\n    \"\"\"Get list of supported provider names.\"\"\"\n    return list(set(cls._providers.keys()))\n</code></pre>"},{"location":"references/#cli.models.ModelConfig","title":"<code>ModelConfig</code>  <code>dataclass</code>","text":"<p>Configuration for an LLM model.</p> Source code in <code>cli/models/base.py</code> <pre><code>@dataclass\nclass ModelConfig:\n    \"\"\"Configuration for an LLM model.\"\"\"\n\n    name: str\n    provider: str\n    api_key: str\n    temperature: float = 0.7\n    max_tokens: Optional[int] = None\n</code></pre>"},{"location":"references/#cli.models.anthropic_provider","title":"<code>anthropic_provider</code>","text":"<p>Anthropic Claude provider implementation using LangChain.</p>"},{"location":"references/#cli.models.anthropic_provider.AnthropicProvider","title":"<code>AnthropicProvider</code>","text":"<p>               Bases: <code>BaseLLMProvider</code></p> <p>Anthropic Claude LLM provider.</p> Source code in <code>cli/models/anthropic_provider.py</code> <pre><code>class AnthropicProvider(BaseLLMProvider):\n    \"\"\"Anthropic Claude LLM provider.\"\"\"\n\n    def _create_llm(self) -&gt; BaseChatModel:\n        kwargs = {\n            \"model\": self.config.name,\n            \"anthropic_api_key\": self.config.api_key,\n            \"temperature\": self.config.temperature,\n        }\n        if self.config.max_tokens:\n            kwargs[\"max_tokens\"] = self.config.max_tokens\n        return ChatAnthropic(**kwargs)\n\n    def get_provider_name(self) -&gt; str:\n        return \"Anthropic Claude\"\n</code></pre>"},{"location":"references/#cli.models.base","title":"<code>base</code>","text":"<p>Base interfaces and abstractions for LLM providers.</p>"},{"location":"references/#cli.models.base.ILLMProvider","title":"<code>ILLMProvider</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Interface for LLM providers.</p> Source code in <code>cli/models/base.py</code> <pre><code>class ILLMProvider(ABC):\n    \"\"\"Interface for LLM providers.\"\"\"\n\n    @abstractmethod\n    def generate(self, prompt: str, **kwargs) -&gt; str:\n        \"\"\"Generate a response from the LLM.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_provider_name(self) -&gt; str:\n        \"\"\"Get the name of the provider.\"\"\"\n        pass\n\n    @abstractmethod\n    def validate_config(self) -&gt; bool:\n        \"\"\"Validate the model configuration.\"\"\"\n        pass\n</code></pre>"},{"location":"references/#cli.models.base.ILLMProvider.generate","title":"<code>generate(prompt, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Generate a response from the LLM.</p> Source code in <code>cli/models/base.py</code> <pre><code>@abstractmethod\ndef generate(self, prompt: str, **kwargs) -&gt; str:\n    \"\"\"Generate a response from the LLM.\"\"\"\n    pass\n</code></pre>"},{"location":"references/#cli.models.base.ILLMProvider.get_provider_name","title":"<code>get_provider_name()</code>  <code>abstractmethod</code>","text":"<p>Get the name of the provider.</p> Source code in <code>cli/models/base.py</code> <pre><code>@abstractmethod\ndef get_provider_name(self) -&gt; str:\n    \"\"\"Get the name of the provider.\"\"\"\n    pass\n</code></pre>"},{"location":"references/#cli.models.base.ILLMProvider.validate_config","title":"<code>validate_config()</code>  <code>abstractmethod</code>","text":"<p>Validate the model configuration.</p> Source code in <code>cli/models/base.py</code> <pre><code>@abstractmethod\ndef validate_config(self) -&gt; bool:\n    \"\"\"Validate the model configuration.\"\"\"\n    pass\n</code></pre>"},{"location":"references/#cli.models.base.ModelConfig","title":"<code>ModelConfig</code>  <code>dataclass</code>","text":"<p>Configuration for an LLM model.</p> Source code in <code>cli/models/base.py</code> <pre><code>@dataclass\nclass ModelConfig:\n    \"\"\"Configuration for an LLM model.\"\"\"\n\n    name: str\n    provider: str\n    api_key: str\n    temperature: float = 0.7\n    max_tokens: Optional[int] = None\n</code></pre>"},{"location":"references/#cli.models.base_provider","title":"<code>base_provider</code>","text":"<p>Base provider implementation with common functionality.</p>"},{"location":"references/#cli.models.base_provider.BaseLLMProvider","title":"<code>BaseLLMProvider</code>","text":"<p>               Bases: <code>ILLMProvider</code></p> <p>Base implementation for LLM providers.</p> Source code in <code>cli/models/base_provider.py</code> <pre><code>class BaseLLMProvider(ILLMProvider):\n    \"\"\"Base implementation for LLM providers.\"\"\"\n\n    def __init__(self, config: ModelConfig):\n        self.config = config\n        self.validate_config()\n        self.llm: BaseChatModel = self._create_llm()\n\n    def _create_llm(self) -&gt; BaseChatModel:\n        \"\"\"Create LLM instance. Must be implemented by subclasses.\"\"\"\n        raise NotImplementedError(\"Subclasses must implement _create_llm\")\n\n    def generate(self, prompt: str, **kwargs) -&gt; str:\n        \"\"\"Generate response from LLM.\"\"\"\n        if \"temperature\" in kwargs:\n            self.llm.temperature = kwargs[\"temperature\"]\n        response = self.llm.invoke(prompt)\n        return response.content\n\n    def validate_config(self) -&gt; bool:\n        \"\"\"Validate configuration.\"\"\"\n        if not self.config.api_key:\n            raise ValueError(f\"API key is required for {self.get_provider_name()}\")\n        if not self.config.name:\n            raise ValueError(\"Model name is required\")\n        return True\n</code></pre>"},{"location":"references/#cli.models.base_provider.BaseLLMProvider.generate","title":"<code>generate(prompt, **kwargs)</code>","text":"<p>Generate response from LLM.</p> Source code in <code>cli/models/base_provider.py</code> <pre><code>def generate(self, prompt: str, **kwargs) -&gt; str:\n    \"\"\"Generate response from LLM.\"\"\"\n    if \"temperature\" in kwargs:\n        self.llm.temperature = kwargs[\"temperature\"]\n    response = self.llm.invoke(prompt)\n    return response.content\n</code></pre>"},{"location":"references/#cli.models.base_provider.BaseLLMProvider.validate_config","title":"<code>validate_config()</code>","text":"<p>Validate configuration.</p> Source code in <code>cli/models/base_provider.py</code> <pre><code>def validate_config(self) -&gt; bool:\n    \"\"\"Validate configuration.\"\"\"\n    if not self.config.api_key:\n        raise ValueError(f\"API key is required for {self.get_provider_name()}\")\n    if not self.config.name:\n        raise ValueError(\"Model name is required\")\n    return True\n</code></pre>"},{"location":"references/#cli.models.factory","title":"<code>factory</code>","text":"<p>Factory for creating LLM provider instances.</p>"},{"location":"references/#cli.models.factory.LLMProviderFactory","title":"<code>LLMProviderFactory</code>","text":"<p>Factory for creating LLM provider instances.</p> Source code in <code>cli/models/factory.py</code> <pre><code>class LLMProviderFactory:\n    \"\"\"Factory for creating LLM provider instances.\"\"\"\n\n    _providers: Dict[str, Type[ILLMProvider]] = {\n        \"google\": GeminiProvider,\n        \"gemini\": GeminiProvider,\n        \"openai\": OpenAIProvider,\n        \"anthropic\": AnthropicProvider,\n        \"claude\": AnthropicProvider,\n    }\n\n    _model_patterns = {\n        \"gemini\": \"google\",\n        \"gpt\": \"openai\",\n        \"claude\": \"anthropic\",\n    }\n\n    @classmethod\n    def create(cls, config: ModelConfig) -&gt; ILLMProvider:\n        \"\"\"Create an LLM provider instance based on configuration.\"\"\"\n        provider_key = config.provider.lower()\n\n        if provider_key in cls._providers:\n            return cls._providers[provider_key](config)\n\n        inferred_provider = cls._infer_provider_from_model(config.name)\n        if inferred_provider:\n            return cls._providers[inferred_provider](config)\n\n        raise ValueError(\n            f\"Unsupported provider: {config.provider}. \"\n            f\"Available: {', '.join(cls.get_supported_providers())}\"\n        )\n\n    @classmethod\n    def _infer_provider_from_model(cls, model_name: str) -&gt; str | None:\n        \"\"\"Infer provider from model name.\"\"\"\n        model_lower = model_name.lower()\n        for pattern, provider in cls._model_patterns.items():\n            if pattern in model_lower:\n                return provider\n        return None\n\n    @classmethod\n    def get_supported_providers(cls) -&gt; list[str]:\n        \"\"\"Get list of supported provider names.\"\"\"\n        return list(set(cls._providers.keys()))\n</code></pre>"},{"location":"references/#cli.models.factory.LLMProviderFactory.create","title":"<code>create(config)</code>  <code>classmethod</code>","text":"<p>Create an LLM provider instance based on configuration.</p> Source code in <code>cli/models/factory.py</code> <pre><code>@classmethod\ndef create(cls, config: ModelConfig) -&gt; ILLMProvider:\n    \"\"\"Create an LLM provider instance based on configuration.\"\"\"\n    provider_key = config.provider.lower()\n\n    if provider_key in cls._providers:\n        return cls._providers[provider_key](config)\n\n    inferred_provider = cls._infer_provider_from_model(config.name)\n    if inferred_provider:\n        return cls._providers[inferred_provider](config)\n\n    raise ValueError(\n        f\"Unsupported provider: {config.provider}. \"\n        f\"Available: {', '.join(cls.get_supported_providers())}\"\n    )\n</code></pre>"},{"location":"references/#cli.models.factory.LLMProviderFactory.get_supported_providers","title":"<code>get_supported_providers()</code>  <code>classmethod</code>","text":"<p>Get list of supported provider names.</p> Source code in <code>cli/models/factory.py</code> <pre><code>@classmethod\ndef get_supported_providers(cls) -&gt; list[str]:\n    \"\"\"Get list of supported provider names.\"\"\"\n    return list(set(cls._providers.keys()))\n</code></pre>"},{"location":"references/#cli.models.gemini_provider","title":"<code>gemini_provider</code>","text":"<p>Google Gemini provider implementation using LangChain.</p>"},{"location":"references/#cli.models.gemini_provider.GeminiProvider","title":"<code>GeminiProvider</code>","text":"<p>               Bases: <code>BaseLLMProvider</code></p> <p>Google Gemini LLM provider.</p> Source code in <code>cli/models/gemini_provider.py</code> <pre><code>class GeminiProvider(BaseLLMProvider):\n    \"\"\"Google Gemini LLM provider.\"\"\"\n\n    def _create_llm(self) -&gt; BaseChatModel:\n        kwargs = {\n            \"model\": self.config.name,\n            \"google_api_key\": self.config.api_key,\n            \"temperature\": self.config.temperature,\n        }\n        if self.config.max_tokens:\n            kwargs[\"max_output_tokens\"] = self.config.max_tokens\n        return ChatGoogleGenerativeAI(**kwargs)\n\n    def get_provider_name(self) -&gt; str:\n        return \"Google Gemini\"\n</code></pre>"},{"location":"references/#cli.models.openai_provider","title":"<code>openai_provider</code>","text":"<p>OpenAI provider implementation using LangChain.</p>"},{"location":"references/#cli.models.openai_provider.OpenAIProvider","title":"<code>OpenAIProvider</code>","text":"<p>               Bases: <code>BaseLLMProvider</code></p> <p>OpenAI LLM provider.</p> Source code in <code>cli/models/openai_provider.py</code> <pre><code>class OpenAIProvider(BaseLLMProvider):\n    \"\"\"OpenAI LLM provider.\"\"\"\n\n    def _create_llm(self) -&gt; BaseChatModel:\n        kwargs = {\n            \"model\": self.config.name,\n            \"openai_api_key\": self.config.api_key,\n            \"temperature\": self.config.temperature,\n        }\n        if self.config.max_tokens:\n            kwargs[\"max_tokens\"] = self.config.max_tokens\n        return ChatOpenAI(**kwargs)\n\n    def get_provider_name(self) -&gt; str:\n        return \"OpenAI\"\n</code></pre>"},{"location":"references/#cli.prompt","title":"<code>prompt</code>","text":"<p>CLI entry point for Vertex-CLI.</p>"},{"location":"references/#cli.prompt.main","title":"<code>main()</code>","text":"<p>Main CLI entry point.</p> Source code in <code>cli/prompt.py</code> <pre><code>def main():\n    \"\"\"Main CLI entry point.\"\"\"\n    raw = sys.argv[1:]\n    known_cmds = [\"chat\", \"debug\", \"config\", \"list\", \"remove\", \"select\"]\n\n    # Initialize services\n    config_manager = ConfigurationManager()\n    llm_service = LLMService(config_manager)\n    history = ChatHistory(HISTORY_FILE)\n\n    # Setup shortcut\n    if raw and raw[0] == \"--setup\":\n        config_manager._create_default_config()\n        print(\"Default configuration created.\")\n        return\n\n    # Default chat if no subcommand\n    if raw and raw[0] not in known_cmds:\n        prompt_text = \" \".join(raw)\n        generate_response(prompt_text, llm_service, history)\n        return\n\n    # Subcommand parsing\n    parser = argparse.ArgumentParser(\n        prog=\"tex\", description=\"CLI for interacting with multiple LLMs via LangChain\"\n    )\n    parser.add_argument(\"--setup\", action=\"store_true\", help=\"Create default config\")\n    subparsers = parser.add_subparsers(dest=\"command\")\n\n    # chat\n    chat_parser = subparsers.add_parser(\"chat\", help=\"Send a prompt to the LLM\")\n    chat_parser.add_argument(\"text\", nargs=\"+\", help=\"Prompt text\")\n\n    # debug\n    debug_parser = subparsers.add_parser(\"debug\", help=\"Debug recent bash commands\")\n    debug_parser.add_argument(\n        \"-n\",\n        \"--number\",\n        type=int,\n        default=DEFAULT_BASH_HISTORY_COUNT,\n        help=\"Number of recent commands\",\n    )\n    debug_parser.add_argument(\"-p\", \"--prompt\", type=str, help=\"Additional explanation prompt\")\n\n    # config\n    config_parser = subparsers.add_parser(\n        \"config\", help=\"Configure a model (usage: config &lt;model&gt; &lt;api_key&gt;)\"\n    )\n    config_parser.add_argument(\"model\", help=\"Model name (e.g., gemini-2.5-flash, gpt-4)\")\n    config_parser.add_argument(\"key\", help=\"API key\")\n    config_parser.add_argument(\n        \"--provider\", type=str, help=\"Provider name (auto-detected if not specified)\"\n    )\n    config_parser.add_argument(\n        \"--temperature\", type=float, default=0.7, help=\"Temperature (0.0-1.0)\"\n    )\n\n    # list\n    subparsers.add_parser(\"list\", help=\"List configured models\")\n\n    # remove\n    remove_parser = subparsers.add_parser(\"remove\", help=\"Remove a configured model\")\n    remove_parser.add_argument(\"model\", help=\"Model name\")\n\n    # select\n    select_parser = subparsers.add_parser(\"select\", help=\"Select active model\")\n    select_parser.add_argument(\"model\", help=\"Model name\")\n\n    args = parser.parse_args(raw)\n\n    if args.setup:\n        config_manager._create_default_config()\n        print(\"Default configuration created.\")\n\n    elif args.command == \"chat\":\n        prompt_text = \" \".join(args.text)\n        generate_response(prompt_text, llm_service, history)\n\n    elif args.command == \"debug\":\n        bash = get_bash_history(args.number)\n        dprompt = f\"{bash}{args.prompt or ''} \"\n        dprompt += \"output what is wrong with the commands used and suggest correct ones\"\n        generate_response(dprompt, llm_service, history)\n\n    elif args.command == \"config\":\n        # Auto-detect provider if not specified\n        provider = args.provider\n        if not provider:\n            model_lower = args.model.lower()\n            if \"gemini\" in model_lower:\n                provider = \"google\"\n            elif \"gpt\" in model_lower or \"openai\" in model_lower:\n                provider = \"openai\"\n            elif \"claude\" in model_lower:\n                provider = \"anthropic\"\n            else:\n                print(f\"Could not auto-detect provider for '{args.model}'\")\n                print(\"Please specify provider with --provider\")\n                print(\"Available: google, openai, anthropic\")\n                return\n\n        config_manager.set_model_config(\n            model_name=args.model,\n            provider=provider,\n            api_key=args.key,\n            temperature=args.temperature,\n        )\n\n    elif args.command == \"list\":\n        print(\"\\nConfigured models:\")\n        print(\"-\" * 60)\n        models = config_manager.list_models()\n        selected = config_manager.get_selected_model()\n\n        if not models:\n            print(\"No models configured.\")\n        else:\n            for name, config in models.items():\n                selected_marker = \" [SELECTED]\" if name == selected else \"\"\n                api_key_status = \"\u2713\" if config.get(\"api_key\") else \"\u2717\"\n                print(f\"{name}{selected_marker}\")\n                print(f\"  Provider: {config.get('provider', 'N/A')}\")\n                print(f\"  API Key: {api_key_status}\")\n                print(f\"  Temperature: {config.get('temperature', 0.7)}\")\n                print()\n\n    elif args.command == \"remove\":\n        try:\n            config_manager.remove_model(args.model)\n        except ValueError as e:\n            print(f\"Error: {e}\")\n\n    elif args.command == \"select\":\n        try:\n            config_manager.set_selected_model(args.model)\n        except ValueError as e:\n            print(f\"Error: {e}\")\n\n    else:\n        parser.print_help()\n</code></pre>"},{"location":"references/#cli.utils","title":"<code>utils</code>","text":""},{"location":"references/#cli.utils.install_requirements","title":"<code>install_requirements()</code>","text":"<p>Install required dependencies.</p> Source code in <code>cli/utils.py</code> <pre><code>def install_requirements():\n    \"\"\"Install required dependencies.\"\"\"\n    dependencies = [\n        \"rich&gt;=14.0.0\",\n        \"langchain&gt;=0.1.0\",\n        \"langchain-google-genai&gt;=1.0.0\",\n        \"langchain-openai&gt;=0.0.5\",\n        \"langchain-anthropic&gt;=0.1.0\",\n    ]\n    for package in dependencies:\n        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package])\n</code></pre>"},{"location":"references/#cli.utils.prettify_llm_output","title":"<code>prettify_llm_output(response)</code>","text":"<p>Prettify LLM output using Rich markdown.</p> Source code in <code>cli/utils.py</code> <pre><code>def prettify_llm_output(response):\n    \"\"\"Prettify LLM output using Rich markdown.\"\"\"\n    console = Console()\n    md = Markdown(response.strip())\n    console.print(\"\\n\", md, \"\\n\")\n</code></pre>"},{"location":"references/#cli.utils.spin_loader","title":"<code>spin_loader(stop_event)</code>","text":"<p>Display a spinning loader.</p> Source code in <code>cli/utils.py</code> <pre><code>def spin_loader(stop_event):\n    \"\"\"Display a spinning loader.\"\"\"\n    spinner = itertools.cycle([\"-\", \"/\", \"|\", \"\\\\\"])\n    while not stop_event.is_set():\n        sys.stdout.write(next(spinner))\n        sys.stdout.flush()\n        time.sleep(0.1)\n        sys.stdout.write(\"\\b\")\n    sys.stdout.write(\" \")\n    sys.stdout.flush()\n</code></pre>"}]}